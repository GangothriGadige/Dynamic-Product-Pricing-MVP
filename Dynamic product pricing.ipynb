{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc0bb26d-831e-44b7-9716-1f2486162302",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sku_id</th><th>category</th><th>manufacturer</th><th>avg_price</th><th>units_sold</th><th>total_purchases</th><th>total_impressions</th><th>cost_price</th><th>competitor_price</th><th>fulfillment_method</th><th>conv_rate</th><th>margin</th><th>sales_rank</th><th>category_count</th><th>is_anchor</th><th>suggested_price</th><th>reason</th></tr></thead><tbody><tr><td>SKU-1001</td><td>Consumables</td><td>Medico</td><td>11.75</td><td>3</td><td>50</td><td>1000</td><td>9.0</td><td>11.0</td><td>Direct</td><td>0.05</td><td>0.3055555555555556</td><td>1</td><td>1</td><td>false</td><td>14.1</td><td>profit_optimized</td></tr><tr><td>SKU-2001</td><td>Instruments</td><td>LabTech</td><td>320.0</td><td>1</td><td>3</td><td>120</td><td>250.0</td><td>310.0</td><td>Supplier</td><td>0.025</td><td>0.28</td><td>1</td><td>1</td><td>false</td><td>384.0</td><td>profit_optimized</td></tr><tr><td>SKU-3001</td><td>Reagents</td><td>ChemLabs</td><td>60.0</td><td>3</td><td>20</td><td>300</td><td>45.0</td><td>58.0</td><td>Direct</td><td>0.06666666666666667</td><td>0.3333333333333333</td><td>1</td><td>1</td><td>false</td><td>72.0</td><td>profit_optimized</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "SKU-1001",
         "Consumables",
         "Medico",
         11.75,
         3,
         50,
         1000,
         9.0,
         11.0,
         "Direct",
         0.05,
         0.3055555555555556,
         1,
         1,
         false,
         14.1,
         "profit_optimized"
        ],
        [
         "SKU-2001",
         "Instruments",
         "LabTech",
         320.0,
         1,
         3,
         120,
         250.0,
         310.0,
         "Supplier",
         0.025,
         0.28,
         1,
         1,
         false,
         384.0,
         "profit_optimized"
        ],
        [
         "SKU-3001",
         "Reagents",
         "ChemLabs",
         60.0,
         3,
         20,
         300,
         45.0,
         58.0,
         "Direct",
         0.06666666666666667,
         0.3333333333333333,
         1,
         1,
         false,
         72.0,
         "profit_optimized"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "sku_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "manufacturer",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "avg_price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "units_sold",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_purchases",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_impressions",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "cost_price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "competitor_price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "fulfillment_method",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "conv_rate",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "margin",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "sales_rank",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "category_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "is_anchor",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "suggested_price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "reason",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Databricks notebook source\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Product metadata\n",
    "# -----------------------------\n",
    "product_data = [\n",
    "    (\"SKU-1001\", \"Consumables\", \"Box of 100\", \"Medico\", \"Direct\"),\n",
    "    (\"SKU-2001\", \"Instruments\", \"Unit\", \"LabTech\", \"Supplier\"),\n",
    "    (\"SKU-3001\", \"Reagents\", \"500ml Bottle\", \"ChemLabs\", \"Direct\")\n",
    "]\n",
    "product_schema = T.StructType([\n",
    "    T.StructField(\"sku_id\", T.StringType(), True),\n",
    "    T.StructField(\"category\", T.StringType(), True),\n",
    "    T.StructField(\"packaging\", T.StringType(), True),\n",
    "    T.StructField(\"manufacturer\", T.StringType(), True),\n",
    "    T.StructField(\"fulfillment_method\", T.StringType(), True)\n",
    "])\n",
    "df_products = spark.createDataFrame(product_data, product_schema)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Transactional data\n",
    "# -----------------------------\n",
    "txn_data = [\n",
    "    (\"SKU-1001\", 12.0, 2, \"2025-09-01\", \"New York\"),\n",
    "    (\"SKU-1001\", 11.5, 1, \"2025-09-05\", \"Boston\"),\n",
    "    (\"SKU-2001\", 320.0, 1, \"2025-09-02\", \"Chicago\"),\n",
    "    (\"SKU-3001\", 60.0, 3, \"2025-09-03\", \"San Francisco\")\n",
    "]\n",
    "txn_schema = T.StructType([\n",
    "    T.StructField(\"sku_id\", T.StringType(), True),\n",
    "    T.StructField(\"price_paid\", T.DoubleType(), True),\n",
    "    T.StructField(\"quantity\", T.IntegerType(), True),\n",
    "    T.StructField(\"timestamp\", T.StringType(), True),\n",
    "    T.StructField(\"customer_location\", T.StringType(), True)\n",
    "])\n",
    "df_txn = spark.createDataFrame(txn_data, txn_schema)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Supplier data\n",
    "# -----------------------------\n",
    "supplier_data = [\n",
    "    (\"SKU-1001\", 9.0, \"Available\", 3),\n",
    "    (\"SKU-2001\", 250.0, \"Limited\", 14),\n",
    "    (\"SKU-3001\", 45.0, \"Available\", 5)\n",
    "]\n",
    "supplier_schema = T.StructType([\n",
    "    T.StructField(\"sku_id\", T.StringType(), True),\n",
    "    T.StructField(\"cost_price\", T.DoubleType(), True),\n",
    "    T.StructField(\"availability\", T.StringType(), True),\n",
    "    T.StructField(\"lead_time_days\", T.IntegerType(), True)\n",
    "])\n",
    "df_supplier = spark.createDataFrame(supplier_data, supplier_schema)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Clickstream data\n",
    "# -----------------------------\n",
    "click_data = [\n",
    "    (\"SKU-1001\", 500, 50, 25),   # impressions, add_to_cart, conversions\n",
    "    (\"SKU-2001\", 120, 10, 3),\n",
    "    (\"SKU-3001\", 300, 40, 20)\n",
    "]\n",
    "click_schema = T.StructType([\n",
    "    T.StructField(\"sku_id\", T.StringType(), True),\n",
    "    T.StructField(\"impressions\", T.IntegerType(), True),\n",
    "    T.StructField(\"add_to_cart\", T.IntegerType(), True),\n",
    "    T.StructField(\"conversions\", T.IntegerType(), True)\n",
    "])\n",
    "df_clicks = spark.createDataFrame(click_data, click_schema)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Market data (competitor pricing)\n",
    "# -----------------------------\n",
    "market_data = [\n",
    "    (\"SKU-1001\", 11.0),\n",
    "    (\"SKU-2001\", 310.0),\n",
    "    (\"SKU-3001\", 58.0)\n",
    "]\n",
    "market_schema = T.StructType([\n",
    "    T.StructField(\"sku_id\", T.StringType(), True),\n",
    "    T.StructField(\"competitor_price\", T.DoubleType(), True)\n",
    "])\n",
    "df_market = spark.createDataFrame(market_data, market_schema)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Join all data sources\n",
    "# -----------------------------\n",
    "df_combined = (\n",
    "    df_txn\n",
    "    .join(df_products, \"sku_id\", \"left\")\n",
    "    .join(df_supplier, \"sku_id\", \"left\")\n",
    "    .join(df_clicks, \"sku_id\", \"left\")\n",
    "    .join(df_market, \"sku_id\", \"left\")\n",
    ")\n",
    "\n",
    "# Derived features\n",
    "df_combined = (\n",
    "    df_combined\n",
    "    .withColumn(\"conv_rate\", F.col(\"conversions\") / F.col(\"impressions\"))\n",
    "    .withColumn(\"margin\", (F.col(\"price_paid\") - F.col(\"cost_price\")) / F.col(\"cost_price\"))\n",
    "    .withColumn(\"price_delta_vs_competitor\", F.col(\"price_paid\") - F.col(\"competitor_price\"))\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Aggregate by SKU\n",
    "# -----------------------------\n",
    "df_agg = (\n",
    "    df_combined\n",
    "    .groupBy(\"sku_id\", \"category\", \"manufacturer\")\n",
    "    .agg(\n",
    "        F.avg(\"price_paid\").alias(\"avg_price\"),\n",
    "        F.sum(\"quantity\").alias(\"units_sold\"),\n",
    "        F.sum(\"conversions\").alias(\"total_purchases\"),\n",
    "        F.sum(\"impressions\").alias(\"total_impressions\"),\n",
    "        F.first(\"cost_price\").alias(\"cost_price\"),\n",
    "        F.first(\"competitor_price\").alias(\"competitor_price\"),\n",
    "        F.first(\"fulfillment_method\").alias(\"fulfillment_method\")\n",
    "    )\n",
    "    .withColumn(\"conv_rate\", F.col(\"total_purchases\") / F.col(\"total_impressions\"))\n",
    "    .withColumn(\"margin\", (F.col(\"avg_price\") - F.col(\"cost_price\")) / F.col(\"cost_price\"))\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Anchor detection\n",
    "# -----------------------------\n",
    "w = Window.partitionBy(\"category\").orderBy(F.desc(\"units_sold\"))\n",
    "df_agg = (\n",
    "    df_agg\n",
    "    .withColumn(\"sales_rank\", F.row_number().over(w))\n",
    "    .withColumn(\"category_count\", F.count(\"sku_id\").over(Window.partitionBy(\"category\")))\n",
    "    .withColumn(\"is_anchor\", F.col(\"sales_rank\") <= (F.col(\"category_count\") * 0.10))\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 9. Pricing logic (UDF)\n",
    "# -----------------------------\n",
    "def suggest_price(cost_price, current_price, competitor_price, conv_rate, category, is_anchor):\n",
    "    min_margin = {\n",
    "        \"Consumables\": 0.05,\n",
    "        \"Instruments\": 0.20,\n",
    "        \"Reagents\": 0.10\n",
    "    }.get(category, 0.10)\n",
    "    \n",
    "    floor_price = cost_price * (1 + min_margin)\n",
    "    \n",
    "    if is_anchor:\n",
    "        suggested = max(floor_price, min(current_price, competitor_price - 0.01))\n",
    "        reason = \"anchor_competitive\"\n",
    "    else:\n",
    "        candidates = [current_price * (1 + p) for p in [-0.2, -0.1, 0, 0.1, 0.2]]\n",
    "        best_price, best_profit = current_price, -1e9\n",
    "        for p in candidates:\n",
    "            if p < floor_price:\n",
    "                continue\n",
    "            price_change_pct = (p - current_price) / current_price\n",
    "            est_conv = max(0.0, conv_rate * (1 - 1.0 * price_change_pct))\n",
    "            exp_profit = (p - cost_price) * est_conv\n",
    "            if exp_profit > best_profit:\n",
    "                best_price, best_profit = p, exp_profit\n",
    "        suggested = best_price\n",
    "        reason = \"profit_optimized\"\n",
    "    \n",
    "    return float(round(suggested, 2)), reason\n",
    "\n",
    "suggest_udf = F.udf(suggest_price, T.StructType([\n",
    "    T.StructField(\"suggested_price\", T.DoubleType()),\n",
    "    T.StructField(\"reason\", T.StringType())\n",
    "]))\n",
    "\n",
    "df_suggest = df_agg.withColumn(\n",
    "    \"pricing\",\n",
    "    suggest_udf(\n",
    "        F.col(\"cost_price\"),\n",
    "        F.col(\"avg_price\"),\n",
    "        F.col(\"competitor_price\"),\n",
    "        F.col(\"conv_rate\"),\n",
    "        F.col(\"category\"),\n",
    "        F.col(\"is_anchor\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_suggest = (\n",
    "    df_suggest\n",
    "    .withColumn(\"suggested_price\", F.col(\"pricing.suggested_price\"))\n",
    "    .withColumn(\"reason\", F.col(\"pricing.reason\"))\n",
    "    .drop(\"pricing\")\n",
    ")\n",
    "\n",
    "display(df_suggest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eed503e-6799-4721-a1a7-c6bf90ac1592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3928240483908113>:15\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m (\n",
       "\u001B[1;32m      6\u001B[0m     df_suggest\u001B[38;5;241m.\u001B[39mwrite\n",
       "\u001B[1;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      8\u001B[0m     \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39msave(output_path)\n",
       "\u001B[1;32m     10\u001B[0m )\n",
       "\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# -----------------------------\u001B[39;00m\n",
       "\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# 11. Read back & display\u001B[39;00m\n",
       "\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# -----------------------------\u001B[39;00m\n",
       "\u001B[0;32m---> 15\u001B[0m df_out \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mload(output_path)\n",
       "\u001B[1;32m     17\u001B[0m display(\n",
       "\u001B[1;32m     18\u001B[0m     df_out\u001B[38;5;241m.\u001B[39mselect(\n",
       "\u001B[1;32m     19\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msku_id\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcategory\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mavg_price\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msuggested_price\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis_anchor\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmargin\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreason\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     20\u001B[0m     )\n",
       "\u001B[1;32m     21\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:302\u001B[0m, in \u001B[0;36mDataFrameReader.load\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n",
       "\u001B[1;32m    300\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n",
       "\u001B[1;32m    301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mstr\u001B[39m):\n",
       "\u001B[0;32m--> 302\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [PATH_NOT_FOUND] Path does not exist: dbfs:/FileStore/pricing_suggestions_parquet_v1."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-3928240483908113>:15\u001B[0m\n\u001B[1;32m      5\u001B[0m (\n\u001B[1;32m      6\u001B[0m     df_suggest\u001B[38;5;241m.\u001B[39mwrite\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39msave(output_path)\n\u001B[1;32m     10\u001B[0m )\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# -----------------------------\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# 11. Read back & display\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# -----------------------------\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m df_out \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mload(output_path)\n\u001B[1;32m     17\u001B[0m display(\n\u001B[1;32m     18\u001B[0m     df_out\u001B[38;5;241m.\u001B[39mselect(\n\u001B[1;32m     19\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msku_id\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcategory\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mavg_price\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msuggested_price\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis_anchor\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmargin\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreason\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     20\u001B[0m     )\n\u001B[1;32m     21\u001B[0m )\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:302\u001B[0m, in \u001B[0;36mDataFrameReader.load\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n\u001B[1;32m    301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m--> 302\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [PATH_NOT_FOUND] Path does not exist: dbfs:/FileStore/pricing_suggestions_parquet_v1.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [PATH_NOT_FOUND] Path does not exist: dbfs:/FileStore/pricing_suggestions_parquet_v1.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 10. Save results to /tmp (Parquet)\n",
    "# -----------------------------\n",
    "output_path = \"dbfs:/FileStore/pricing_suggestions_parquet_v1\"\n",
    "(\n",
    "    df_suggest.write\n",
    "    .format(\"parquet\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(output_path)\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 11. Read back & display\n",
    "# -----------------------------\n",
    "df_out = spark.read.format(\"parquet\").load(output_path)\n",
    "\n",
    "display(\n",
    "    df_out.select(\n",
    "        \"sku_id\",\"category\",\"avg_price\",\"suggested_price\",\"is_anchor\",\"margin\",\"reason\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a065c1f-28d2-442b-b1a6-f958b8058a86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Dynamic product pricing",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}